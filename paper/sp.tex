%% This file is part of the spectroscopic_parallax project
%% Copyright 2018 the authors.

% To-do items
% -----------
% - draft introduction
% - draft discussion
% - make sure we address the sensibleness and generality of the linear model for the photometric space
% - address the non-convexity of the model (see twitter convo with @andy_l_jones).
% - Coryn's point that we should carefully explain how our model is geometric.
% - related: HWR's point that this is standardizeable candles, like the Ia SNe results.
% - related: Put in an explanation of WHY this works so well.
% - related: Put in an explanation of how it is that we aren't really modeling luminosity or distance but really just generating parallaxes.
% - does the model need a cute name, like The Swan?
% - we need a Facilities list / tags
% - we need a Software list / tags
% - check with WISE experts that we have the correct WISE photometry

\documentclass[modern]{aastex62}
\usepackage{amsmath}

%% Reintroduced the \received and \accepted commands from AASTeX v5.2
%\received{TBD}
%\revised{TBD}
%\accepted{TBD}
%% Command to document which AAS Journal the manuscript was submitted to.
%% Adds "Submitted to " the arguement.
%\submitjournal{AAS Journals}

\shorttitle{spectrophotometric parallax}
\shortauthors{hogg, eilers, rix}
%\watermark{kittens}

% text macros
\newcommand{\code}[1]{\texttt{\detokenize{#1}}}
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\apogee}{\project{\acronym{APOGEE}}}
\newcommand{\gaia}{\project{Gaia}}
\newcommand{\wise}{\project{\acronym{WISE}}}
\newcommand{\zmass}{\project{\acronym{2MASS}}}
\newcommand{\sdssiv}{\project{\acronym{SDSS-IV}}}
\newcommand{\sdssv}{\project{\acronym{SDSS-V}}}

% math macros
\newcommand{\T}{^{\mathsf{T}}}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\logg}{\log g}
\newcommand{\gparallax}{\varpi^{(\mathrm{a})}}
\newcommand{\sparallax}{\varpi^{(\mathrm{sp})}}

% tweak the modern style -- trust in Hogg
\setlength{\parindent}{1.0\baselineskip}
\addtolength{\textheight}{0.5in}
\addtolength{\topmargin}{-0.3in}

\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing % trust in Hogg

\title{\textbf{%
Spectrophotometric parallaxes with \project{The Cygnet}:\\
Accurate estimates of luminosity and distance\\
for luminous red-giant stars
}}

\author[0000-0003-2866-9403]{David W. Hogg}
\affiliation{Center for Cosmology and Particle Physics, Department of Physics, New York University, 726 Broadway, New York, NY 10003, USA}
\affiliation{Center for Data Science, New York University, 60 Fifth Ave, New York, NY 10011}
\affiliation{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, 69117 Heidelberg, Germany}
\affiliation{Flatiron Institute, 162 Fifth Ave, New York, NY 10010, USA}

\author[0000-0003-2895-6218]{Anna-Christina Eilers}
\affiliation{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, 69117 Heidelberg, Germany}
\affiliation{International Max Planck Research School for Astronomy \& Cosmic Physics at the University of Heidelberg}

\author[0000-0003-4996-9069]{Hans-Walter Rix}
\affiliation{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, 69117 Heidelberg, Germany}

\begin{abstract}\noindent
With contemporary infrared spectroscopic surveys like \apogee,
red-giant stars can be observed to distances and extinctions
at which geometric parallaxes are not precisely measured by \gaia.
Here we use the \apogee--\gaia\ overlap to train a purely linear model of
continuum-normalized \apogee\ spectroscopy
(plus \gaia, \zmass, and \wise\ photometry)
that predicts distance modulus (logarithmic parallax or distance)
for red stars with $0<\logg<2.2$.
The training makes use of a justified likelihood function for the \gaia\ parallaxes,
so that the training can be performed with full use of uncertainties and
without any cuts on parallax or parallax signal-to-noise ratio (not even any non-negative
requirements).
There are no physical inputs to the model; it is a pure regression designed
to accurately predict astrometric parallaxes.
The model includes an L1 regularization that zeros out the contributions of
most spectral pixels to the parallax estimates.
The training is performed
with leave-out subsamples such that no star's astrometry is used even indirectly in its
spectrophotometric parallax estimate.
The model is flexible enough---and has enough photometric information---to
correct for extinction by dust without any
input of explicit extinction or reddening estimates.
We re-label each star in the sample
with a new spectrophotometric parallax; this new parallax has an empirical (1-sigma)
uncertainty of 9~percent, which is more precise than the \gaia\ parallax
for the vast majority of targets.
Validation with globular and open clusters shows that the spectrophotometric parallaxes
are both accurate and precise.
These distance estimates open up great opportunities for
mapping the Milky Way disk with the next
generation of spectroscopic surveys, and especially \sdssv.
\end{abstract}

\keywords{%
methods:~statistical
 ---
techniques:~spectroscopic
 ---
catalogs
 ---
surveys
% ---
%parallaxes
 ---
stars:~distances
 ---
Galaxy:~disk
 ---
infrared:~stars
}

\section{Introduction} \label{sec:intro}

If we want to make precise kinematic and element-abundance maps
of the Milky Way disk out to large heliocentric radii,
and especially through the extinction to the other side of the Galactic Center, we will
need to make use of luminous red giants, and we will need to observe in
the infrared.
These arguments motivate the \apogee\ (CITE), \apogee-2 (CITE), and \sdssv\ (CITE)
projects, which take spectra in the infrared, and deliver detailed abundances
along the entire red-giant branch up to the tip.
The maps made with these data will reveal the dynamics of the Milky Way and its disk,
and show us how and where stars form, and how they migrate around (and out of) the
disk with time.

As we operate these spectroscopic surveys,
the \gaia\ Mission has been revolutionizing our view of the disk.
It delivers good proper motions everywhere,
and extremely valuable parallax information
locally.
However, it does not deliver precise geometric parallaxes over a large fraction of
the disk, and certainly not in the dusty and crowded regions.
For these reasons, the value of \gaia\ in these infrared spectroscopy projects is not
to deliver distance information directly, but rather to calibrate stellar models,
which then deliver distance information through relationships between stellar
luminosities and spectral characteristics.

In this contribution we develop data-driven techniques to predict
stellar luminosities and distances with spectroscopy and photometry...

The history and value (and disadvantages)
of data-driven models for these kinds of problems.
How could it be that we are completely independent of stellar models?

The dangers of aggressive machine-learning methods. Why will we just
look at linear models? The disadvantages of this approach.

The dangers of cutting on \gaia\ properties. Why we will not cut on
\gaia\ parallaxes. The disadvantages of this approach.

\section{Why and how should we model red-giant stars?}

HOGG: We need to, for \sdssv.

HOGG: We know we can because of physical arguments.

HOGG: What kinds of inputs do we need to predict parallaxes?

\section{Assumptions of the method}

Our position is that a methodological technique is correct inasmuch as
it delivers correct or best results \emph{under its particular assumptions}.
In that spirit, we present here the assumptions of the method
explicitly.
We will return to these assumptions in the Discussion section below,
to address the costs and benefits of each in more depth.
\begin{description}
\item[features] Perhaps our most fundamental assumption is that the parallax
of a star can be predicted from the features we provide, which are
the full set of (pseudo-continuum-normalized) pixels from the \apogee\ spectrum,
plus the $G$ and $({BP}-{RP})$ photometry from \gaia,
plus the $J$, $H$, and $K_s$ photometry from \zmass,
plus the $W_1$ and $W_2$ photometry from \wise.
That is, we assume that these spectrophotometric \emph{features} are sufficient
to predict the parallax in the face of variations in stellar
age, evolutionary phase, composition, and other parameters, and also interstellar
extinction.

\item[good features] We assume that the spectrophotometric features are known for
each star with such high fidelity (both precision and accuracy) that we do not
need to account for errors or uncertainties or biases in the features.
That is, we assume that the features are substantially higher in signal-to-noise than the
quantities we are trying to predict; in particular we are assuming that the photometry
and the spectroscopy is better measured than the astrometry.
This is true for most features for most stars, but it does not hold universally.

\item[representativeness] We assume that the training set constructed from the overlap
of \gaia\ and \apogee\ data sets constitutes a representative sample of stars,
sufficiently representative to train the model for all other stars.
Although this assumption is not terrible, it has a weakness:
The stars at greatest distances and greatest local (angular) stellar crowding have
the least precise \gaia\ parallax measurements, and therefore will effectively get
less weight in the fits performed to train the model.

\item[sparsity] We expect that only a small subset of the full complement of
\apogee\ spectral pixels will be relevant to the prediction of parallax
(the spectrophotometric parallax).
That is, we expect that many of the pixels will or ought to get no weight in the
final model that we use to predict luminosities and distances.

\item[linearity] Perhaps the most restrictive assumption of this work is that
the logarithmic parallax (or, equivalently, the distance modulus) 
can be predicted as a \emph{completely linear function} of
the chosen features. We are only assuming this linearity in a small range of stellar
surface gravity $\logg$, but this assumption is strong, and limits strongly the
flexibility or capacity of the model.
We make it to ensure that our method is easy to optimize, and the results are easy
to interpret.

\item[likelihood] We assume that the \gaia\ parallaxes are generated by a particular
stochastic process, in which the difference between the \gaia-Catalog parallax (plus
a small \gaia-recommended offset HOGG CITE) and the true parallax is effectively drawn from a
Gaussian with a width set by the \gaia-Catalog uncertainty on the parallax.
This is the standard assumption in all properly probabilistic \gaia-Mission inferences
to date, but it subsumes a number of related assumptions, like that the \gaia\ noise
model is correct, that the stars are only covariant at negligible levels, and that
there are no significant outliers or large-scale systematics.
\end{description}

In addition to making the above assumptions, we also avoid various practices
with the data that are tempting but lead to strong biases.
For example, we never cut on \gaia\ parallax or parallax signal-to-noise.
The common practices of cutting to parallaxes that are good to 20~percent,
or Catalog parallaxes that are positive, or parallaxes that are smaller or
larger than something, are all practices that will bias results on stellar
collections.
That is, if you cut on parallax or parallax signal-to-noise and you subsequently
take an average of parallaxes for some population, or perform a regression (as we
do here), the results of that average or regression will be strongly biased.
We never cut on parallax or parallax signal-to-noise.
By using a justifiable likelihood function (the ``likelihood'' assumption above),
we can use all of the \gaia\ parallaxes without the low signal-to-noise and
negative parallaxes causing trouble for our regressions, and without the biases
that enter when cuts are made.

Along similar lines, we never assume that the distance is the inverse of the
measured parallax.
In what follows, a star's distance is a latent property of the star, which generates
the \gaia\ Catalog parallax through a noisy process (again, this is
the ``likelihood'' assumption above).
We never take the inverse or the logarithm of the measured parallax at any time.
This is related to the no-cuts point above:
If you take the inverse or logarithm of the parallax, you can't operate safely
on the negative and low signal-to-noise parallaxes, which in turn will require
making cuts on the sample, which will in turn bias the results.

Finally, we never use Lutz--Kelker corrections (HOGG CITE) or distance
posteriors (HOGG CITE). These both involve (implicit or explicit) priors on 
distance.
When multiple stars are combined as we combine them here (below),
use of distance posteriors instead of parallax likelihoods is not just
unjustified, but it also leads to an
effective raising of the distance prior to an enormous power.
That is, the effective prior on a $N$-star inference performed naively
with distance posteriors (made with a weak prior) can end up
bringing into the inference an \emph{exceedingly
strong prior}.
Therefore we don't use any prior-contaminated parallax or distance
inputs to the inference.

\section{Method}

There are not many choices
for building a model of the stars that is both justifiable probabilistically
and consistent with the assumptions stated in the previous Section.
Here we lay out the model and methodology.
We apply the method to real data in the following Sections.

The model and log-likelihood function can be expressed heuristically as
\begin{eqnarray}
\gparallax_n &=& \exp(\theta\cdot x_n) + \mbox{noise}
\\
\chi^2(\theta) &\equiv& \sum_{n=1}^N \frac{[\gparallax_n - \exp(\theta\cdot x_n)]^2}{\sigma_n^2}
\quad ,
\end{eqnarray}
where
$\gparallax_n$ is the \gaia\ measurement (adjusted; see below) of the parallax of star $n$,
the model is that the logarithm of the true parallax
can be expressed as a linear combination of the components
of some $D$-dimensional feature vector $x_n$,
$\theta$ is a $D$-vector of linear coefficients,
$\sigma_n$ is the \gaia\ estimate of the uncertainty on the parallax measurement,
and $\chi^2(\theta)$ is (twice) the negative-log-likelihood for the parameters $\theta$
under the assumption of known Gaussian noise and
that there are $N$ independently measured stars $n$.
The feature vector $x_n$ contains photometry in a few bands, and all 7400-ish pixels
in the pseudo-continuum-normalized \apogee\ spectrum, so $D$ is on the order of 7400.

In addition, we assume that many entries in the $D$-vector will be zero
(the ``sparsity'' assumption).
In order to encourage this, we optimize not $\chi^2$ but a regularized objective function
\begin{eqnarray}
\hat{\theta} &\leftarrow& \argmin_{\theta}\left[\frac{1}{2}\,\chi^2(\theta) + ||\lambda\cdot\theta||_1^1\right]
\quad ,
\end{eqnarray}
where
$\lambda$ is a $D$-vector of regularization parameters (such that we can regularize
different parts of the $x_n$ vectors differently),
and $||x||_1^1$ is the L1-norm or sum of absolute values of the components of $x$.
This kind of regularization adds a convex term to the optimization and leads to
sparse optima.
We will set the value $\lambda$ of the $D$-vector of regularization strengths by a
very coarse cross-validation.

Once the model is optimized, the output of the model is a prediction of the parallax,
or really what we will call the \emph{spectrophotometric parallax}.
This spectrophotometric parallax
$\sparallax_m$ for any star $m$ in the validation or test set is
assigned according to
\begin{eqnarray}
\sparallax_m &\leftarrow& \exp(\hat{\theta}\cdot x_m)
\quad ,
\end{eqnarray}
where
$\hat{\theta}$ is the optimal parameter vector according to (HOGG WHAT EQUATION),
and
$x_m$ is the feature $D$-vector for star $m$.
Thus the model can be trained on a training set of stars and applied to make
predictions for a validation set or a test set.
The only requirements are that every test-set object $m$  must have a full feature
vector $x_m$ just as every training-set object $n$ must have a full feature
vector $x_n$.

One final detail: We perform all of the fitting in a two-fold train-and-test framework,
in which the data are split into two disjoint subsets, A and B.
The model trained on the A data is used to predict or produce
the spectrophotometric parallaxes $\sparallax_m$ values (and hence
the distances and parallaxes) of the B data,
and the model trained on the B data is used to 
produce the $\sparallax_m$ values of the A data.
This ensures that, in the estimate of any individal star's
parallax, none of the \gaia\ data pertaining to that star were used.
This makes the parallax estimates from the spectrophotometric feature vectors
$x_m$ statistically independent of the \gaia\ parallax measurements.
They are not independent globally---\gaia\ data were used to train the model---but
each star's spectrophotometric parallax estimate is independent
of the astrometric estimate from \gaia\ on a star-by-star basis.

\section{Data}

We take the \apogee\ (HOGG CITE) spectral data
from \sdssiv\ (HOGG CITE) \acronym{DR14} (HOGG CITE).
Because we want to make a purely linear model, which has very little capacity,
we restrict our consideration to a small region in stellar parameter space.
We cut the \apogee\ data down to the range $0<\logg<2.2$, which isolates
stars that are more luminous than the red clump.
The \apogee\ pipeline $\logg$ values (which we use) have uncertainties but
this cut leads to a clean sample of luminous red giants, and it is a cut
that is only on spectral properties of the stars (and not photometry nor astrometry).

From the \apogee\ data on these low-$\logg$ stars, we take the spectral pixels,
of which there are 7405 (after cutting out pixels that rarely or never get data),
on a common rest-frame wavelength grid.
That is, every \apogee\ star is extracted on (or interpolated to)
the same wavelength scale.
In detail, we obtain the
the \apogee\ spectral pixels from the pipeline-generated \code{aspcapStar} files.
We then pseudo-continuum-normalize the spectra according to the procedure developed
in \project{The~Cannon} (HOGG CITE):
That is, the pseudo-continuum is a spline fit to a set
of pixels chosen to be insensitive to stellar parameters.
We use as our spectral data the normalized spectral pixel values.

Because the \apogee\ target selection is based on \zmass, every \apogee\ star
also comes with \zmass\ (HOGG CITE) photometry.
That is, for each star $n$,
we have \zmass\ near-infrared photometry $J_n$, $H_n$, and $K_n$.

We used the \gaia\ Archive (HOGG CITE) to match the \apogee+\zmass\ stars
to the \wise\ Catalog (HOGG CITE),
according to the \gaia\ Archive internal match criteria.
This gives, fore each matching star $n$,
mid-infrared photometry $W_{1n}$ and $W_{2n}$ at 3.6 and 4.5\,$\mu$m
In detail we use the \code{w1mpro} and \code{w2mpro} Catalog entries.

We match this full-match catalog to the \gaia\ \acronym{DR2} (HOGG CITE)
using the \gaia\ Archive official match (given the \zmass\ IDs).
We take from the \gaia\ \acronym{DR2} Catalog the photometric data
$G_n$ (\code{phot_g_mean_mag}),
${BP}_n$ (\code{phot_bp_mean_mag}),
and ${RP}_n$ (\code{phot_rp_mean_mag}),
which will become part of the feature vector $x_n$.

We need complete feature-vector information for all stars.  For this
reason, we define the Parent Sample to be the set of all stars that
meet the \apogee\ and \gaia\ cuts and also
have the complete set of photometry: $G_n$, ${BP}_n$, ${RP}_n$, $J_n$,
$H_n$, $K_n$, $W_{1n}$, and $W_{2n}$ and also an apogee spectrum.
This Parent Sample contains ACE HOW MANY STARS? and is shown in HOGG WHAT FIGURES?

Every Parent-Sample star gets, in addition, a randomly assigned binary
label (A or B).
This is used for two-fold validation and jackknife.
In short, we will use the model trained on the A data to assign luminosities
and distances to the B data and \foreign{vice versa}.
The Parent Sample A has HOGG entries and Parent Sample B has HOGG entries.

For training, we need A and B Training Sets.
We define the Training Set stars to be Parent-Sample stars that also
have a measured \gaia\ parallax.
The Training-Set stars, in addition to meeting all Parent-Sample cuts,
also must meet ACE WHAT ADDITIONAL CUTS to ensure that the parallax
measurements are good.
But---as we have emphasized above---we do not cut ever on parallax or
parallax error or their ratio. For this reason, most of the training set
do not have significantly measured parallaxes and HOGG percent even have
negative parallaxes!
But we will perform our training such that it will be unbiased in these
circumstances.
Indeed it would be necessarily biased if ever we did cut on parallax or
parallax signal-to-noise.

In detail,
for each star $n$ in the Training Sets, we take from \gaia\ DR2
the parallax $\gparallax_n$ and its uncertainty $\sigma_n$.
Importantly, to each \gaia\ parallax measurement we add a positive
offset of $0.029$\,mas to adjust for the under-estimation of
parallaxes reported by the \gaia\ Collaboration (HOGG CITE LINDEGREN).
The training sets are shown in HOGG WHAT FIGURES?

For every star $n$ in the full Parent Sample we construct the feature
vector $x_n$ as
\begin{eqnarray}
x_n\T &\equiv& [1, G_n, {BP}_n, {RP}_n, J_n, H_n, K_n, W_{1n}, W_{2n}, f_{1n}, f_{2n}, \cdots, f_{Ln}]
\quad ,
\end{eqnarray}
where the 1 permits a linear offset,
the photometry is from \gaia, \zmass, and \wise, respectively,
and
the fluxes in the $L=7405$ \apogee\ spectral pixels (for which there are reliably
and consistently data) for star $n$ are denoted
$f_{1n}$, $f_{2n}$, and so on.
These feature vectors live in a $D$-dimensional space where $D=7414$
For every star $n$ in either of the Training Sets we additionally require---in
addition to these feature vectors---a \gaia-measured parallax $\gparallax_n$
and uncertainty $\sigma_n$.

\section{Results and validation}

We optimize two models, one for Training Set A and one for Training Set B.
For optimization, we use \code{scipy} l-bfgs-b (HOGG CHECK), starting at
an uninformed starting point in the $\theta$ space.
The optimization is of the objective function given in (HOGG WHAT EQUATION) above,
and although this optimization is not convex, it appears to converge to the
same good solution quickly and reliably from multiple different starting points.

The two models are applied to the two splits of the Parent Sample, Parent Sample
A and Parent Sample B, using the A-trained model on Parent Sample B and
\foreign{vice versa}.
For the purposes of assessing the accuracy and precision of the model, this
train-and-test framework constitutes a two-fold cross-validation.
Results of this cross-validation is shown in HOGG WHAT FIGURE.
For the stars with highest \gaia-measured parallax signal-to-noise,
the spectrophotometric model is predicting the \gaia\ parallaxes with little bias
and a scatter of better than 9~percent.

We used this cross-validation framework to adjust the regularization parameters.
We (arbitrarily) chose zero regularization on the photometric components of the
$x_n$ vectors, and uniform, identical regularization strength for all of the
spectral (\apogee-pixel) components.
We did a coarse grid search in the value for the spectral-component value of the
$\lambda$ vector, using the A/B split as the two-fold cross-validation.
The 9-percent scatter is what we obtain at the optimal setting of the regularization
strength.

We adopt 9~percent as the method's precision.
This estimate is conservative in that the 9-percent scatter includes contributions
from both our spectrophotometric model and from the \gaia\ measurements themselves.
However this estimate is not so conservative in that the test is performed with
\gaia's best stars, which are bright, nearby, in low extinction regions, and
(because of these things) near Solar metallicity.
There could in principle be additional bias or scatter for stars that are in dustier
regions or at lower metallicities.
\figurename~HOGG WHAT FIGURE shows that
there is no suggestion of any such trends when we color the residuals by metallicity
or $H-W_2$ color (which is a reddening proxy).

Another validation of the results can be obtained by looking at known
clusters or spatially compact objects in the data.
In HOGG WHAT FIGURE, we show the parallax distribution from \gaia\ and
the spectrophotometric-parallax distribution from this work for stars in
angular proximity to known stellar clusters and the Sagittarius dwarf
galaxy.
These clusters span some range in metallicity and age, and therefore test
the accuracy of the method for different kinds of populations.
They show that both \gaia\ and the spectrophotometric parallaxes appear to be unbiased
for these clusters and the range in abundances and ages they represent.
HOGG: DO WE NEED TO ADDRESS whether why not these clusters show 9-percent spreads?

The linear model we use has a big limitation: Linear models are inflexible.
However, it has a great advantage over more general methods,
which is that it is easy to look inside
the linear model and check whether the dependencies represented there make sense.
One nice thing about the model is that the feature vector $x_n$ for star $n$
contains a set of photometry in magnitudes.
Linear combinations of these photometric measurements are like complex synthetic
magnitudes or colors.
Since the model was trained to deliver luminosities of stars given training data
of different metallicities and extinctions, the synthetic photometry developed
by the model should be, in some sense, metallicity- and extinction-independent
photometry for luminous red-giant stars!
In detail this synthetic magnitude is something like HOGG PUT EQUATION HERE AND
SAY SOMETHING ABOUT IT. HOGG: Maybe plot this synthetic photometry against dust
and metallicity.

HOGG: How to get the data and how to use it, probabilistically.

\section{Discussion}

What did we do and what's cool?

Why did we make the assumptions we made and what did they gain us and cost us?

(In addition: what did we assume about the \apogee\ data: Good $\logg$ values.
Okay pseudo-continuum normalization!)

(Mention independence of every result from the \gaia\ astrometry.)

What is this useful for and how do you use them safely?
HOGG: Show a map of the kinematic rotation of the Galaxy as a function of
x and y, colored by metallicity.
We see the kinematic center of the Galaxy! We see the rotation curve.
We see the chemical gradients.

How do you build a proper probabilistic model using these? How is the answer to that
related to the answer for \gaia\ itself?

\acknowledgements
It is a pleasure to thank
  Coryn Bailer-Jones (\acronym{MPIA}),
  Andy L. Jones,
  Daniel Michalik (\acronym{ESTEC}),
  Melissa Ness (Columbia),
  and the participants in the \acronym{MPIA} Milky Way Group Meeting
for help with this project.
This project was developed in part at the
2018 \acronym{NYC} Gaia Sprint, hosted by the Center for Computational Astrophysics of
the Flatiron Institute in New York City in 2018 June.

This work has made use of data from the European Space Agency (ESA) mission
\gaia\ (\url{https://www.cosmos.esa.int/gaia}), processed by the \gaia\ Data
Processing and Analysis Consortium (\acronym{DPAC},
\url{https://www.cosmos.esa.int/web/gaia/dpac/consortium}). Funding for the
\acronym{DPAC}
has been provided by national institutions, in particular the institutions
participating in the \gaia\ Multilateral Agreement.

\wise\ Acknowledgement?

\project{\acronym{SDSS-IV}} ack.

\begin{thebibliography}{}
\bibitem[Astropy Collaboration et al.(2013)]{2013A&A...558A..33A} Astropy Collaboration, Robitaille, T.~P., Tollerud, E.~J., et al.\ 2013, \aap, 558, A33 
\bibitem[Bertin \& Arnouts(1996)]{1996A&AS..117..393B} Bertin, E., \& Arnouts, S.\ 1996, \aaps, 117, 393 
\bibitem[Corrales(2015)]{2015ApJ...805...23C} Corrales, L.\ 2015, \apj, 805, 23
\bibitem[Ferland et al.(2013)]{2013RMxAA..49..137F} Ferland, G.~J., Porter, R.~L., van Hoof, P.~A.~M., et al.\ 2013, \rmxaa, 49, 137
\bibitem[Hanisch \& Biemesderfer(1989)]{1989BAAS...21..780H} Hanisch, R.~J., \& Biemesderfer, C.~D.\ 1989, \baas, 21, 780 
\bibitem[Lamport(1994)]{lamport94} Lamport, L. 1994, LaTeX: A Document Preparation System, 2nd Edition (Boston, Addison-Wesley Professional)
\bibitem[Schwarz et al.(2011)]{2011ApJS..197...31S} Schwarz, G.~J., Ness, J.-U., Osborne, J.~P., et al.\ 2011, \apjs, 197, 31  
\bibitem[Vogt et al.(2014)]{2014ApJ...793..127V} Vogt, F.~P.~A., Dopita, M.~A., Kewley, L.~J., et al.\ 2014, \apj, 793, 127  
\end{thebibliography}

%% This command is needed to show the entire author+affilation list when
%% the collaboration and author truncation commands are used.  It has to
%% go at the end of the manuscript.
%\allauthors

%% Include this line if you are using the \added, \replaced, \deleted
%% commands to see a summary list of all changes at the end of the article.
%\listofchanges

\end{document}

% End of file `sample62.tex'.
