%% This file is part of the spectroscopic_parallax project
%% Copyright 2018 the authors.

% To-do items
% -----------
% - draft introduction
% - draft data+results section
% - draft discussion
% - make sure we address the cross-validation point
% - make sure we address the sensibleness and generality of the linear model for the photometric space
% - address the non-convexity of the model (see twitter convo with @andy_l_jones).

\documentclass[modern]{aastex62}
\usepackage{amsmath}

%% Reintroduced the \received and \accepted commands from AASTeX v5.2
%\received{TBD}
%\revised{TBD}
%\accepted{TBD}
%% Command to document which AAS Journal the manuscript was submitted to.
%% Adds "Submitted to " the arguement.
%\submitjournal{AAS Journals}

\shorttitle{spectrophotometric distance estimates}
\shortauthors{hogg, eilers, rix}
%\watermark{kittens}

% text macros
\newcommand{\code}[1]{\texttt{\detokenize{#1}}}
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\apogee}{\project{\acronym{APOGEE}}}
\newcommand{\gaia}{\project{Gaia}}
\newcommand{\wise}{\project{\acronym{WISE}}}
\newcommand{\zmass}{\project{\acronym{2MASS}}}
\newcommand{\sdssv}{\project{\acronym{SDSS-V}}}

% math macros
\newcommand{\logg}{\log g}
\DeclareMathOperator*{\argmin}{argmin}

% tweak the modern style -- trust in Hogg
\setlength{\parindent}{1.0\baselineskip}
\addtolength{\textheight}{0.5in}
\addtolength{\topmargin}{-0.3in}

\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing % trust in Hogg

\title{\textbf{%
Accurate spectrophotometric estimates of luminosity and distance\\
for luminous red-giant stars%
}}

\author[0000-0003-2866-9403]{David W. Hogg}
\affiliation{Center for Cosmology and Particle Physics, Department of Physics, New York University}
\affiliation{Center for Data Science, New York University}
\affiliation{Max-Planck-Institut f\"ur Astronomie}
\affiliation{Flatiron Institute}

\author[0000-0003-2895-6218]{Anna-Christina Eilers}
\affiliation{Max-Planck-Institut f\"ur Astronomie}

\author[0000-0003-4996-9069]{Hans-Walter Rix}
\affiliation{Max-Planck-Institut f\"ur Astronomie}

\begin{abstract}\noindent
With contemporary infrared spectroscopic surveys like \apogee,
red-giant stars can be observed to distances and extinctions
at which geometric parallaxes are not precisely measured by \gaia.
Here we use the \apogee--\gaia\ overlap to train a purely linear model of
continuum-normalized \apogee\ spectroscopy
(plus \gaia, \zmass, and \wise\ photometry)
that predicts absolute magnitude (logarithmic luminosity) for red stars with $0<\logg<2.2$.
The training makes use of a justified likelihood function for the \gaia\ parallaxes,
so that the training can be performed with full use of uncertainties and
without any cuts on parallax or parallax signal-to-noise ratio.
The model includes an L1 regularization that zeros out the contributions of
most spectral pixels to the parallax estimates.
The training is performed
with leave-out subsamples such that no star's astrometry is used even indirectly in its
spectroscopic parallax estimate.
The model is flexible enough---and has enough photometric information---to
correct for extinction by dust without any
input of explicit extinction or reddening estimates.
We re-label each star in the sample
with a new spectroscopic parallax; this new parallax has an empirical (1-sigma)
uncertainty of 9~percent, which is more precise than the \gaia\ parallax
for the vast majority of targets.
Validation with globular and open clusters shows that the spectroscopic parallaxes
are both accurate and precise.
These distance and luminosity estimates open up great opportunities for
mapping the Milky Way disk with the next
generation of spectroscopic surveys, and especially \sdssv.
\end{abstract}

\keywords{%
methods:~statistical
 ---
techniques:~spectroscopic
 ---
catalogs
 ---
surveys
% ---
%parallaxes
 ---
stars:~distances
 ---
Galaxy:~disk
 ---
infrared:~stars
}

\section{Introduction} \label{sec:intro}

If we want to make precise kinematic and element-abundance maps
of the Milky Way disk out to large heliocentric radii,
and especially through the extinction to the other side of the Galactic Center, we will
need to make use of luminous red giants, and we will need to observe in
the infrared.
These arguments motivate the \apogee\ (CITE), \apogee-2 (CITE), and \sdssv\ (CITE)
projects, which take spectra in the infrared, and deliver detailed abundances
along the entire red-giant branch up to the tip.
The maps made with these data will reveal the dynamics of the Milky Way and its disk,
and show us how and where stars form, and how they migrate around (and out of) the
disk with time.

As we operate these spectroscopic surveys,
the \gaia\ Mission has been revolutionizing our view of the disk.
It delivers good proper motions everywhere,
and extremely valuable parallax information
locally.
However, it does not deliver precise geometric parallaxes over a large fraction of
the disk, and certainly not in the dusty and crowded regions.
For these reasons, the value of \gaia\ in these infrared spectroscopy projects is not
to deliver distance information directly, but rather to calibrate stellar models,
which then deliver distance information through relationships between stellar
luminosities and spectral characteristics.

In this contribution we develop data-driven techniques to predict
stellar luminosities and distances with spectroscopy and photometry...

The history and value (and disadvantages)
of data-driven models for these kinds of problems.

The dangers of aggressive machine-learning methods. Why will we just
look at linear models? The disadvantages of this approach.

The dangers of cutting on \gaia\ properties. Why we will not cut on
\gaia\ parallaxes. The disadvantages of this approach.

\section{Assumptions of the method}

Our position is that a methodological technique is correct inasmuch as
it delivers correct or best results \emph{under its particular assumptions}.
In that spirit, we present here the assumptions of the method
explicitly.
We will return to these assumptions in the Discussion section below,
to address the costs and benefits of each in more depth.
\begin{description}
\item[features] Perhaps our most fundamental assumption is that the intrinsic
luminosity of a star can be predicted from the features we provide, which are
the full set of (pseudo-continuum-normalized) pixels from the \apogee\ spectrum,
plus the $G$ and $G_{BP}-G_{RP}$ photometry from \gaia,
plus the $J$, $H$, and $K_s$ photometry from \zmass,
plus the $W_1$ and $W_2$ photometry from \wise.
That is, we assume that these spectrophotometric \emph{features} are sufficient
to predict the intrinsic luminosity in the face of variations in stellar
age, evolutionary phase, composition and other parameters, and also interstellar
extinction.

\item[good features] We assume that the spectrophotometric features are known for
each star with such high fidelity (both precision and accuracy) that we do not
need to account for errors or uncertainties or biases in the features.
That is, we assume that the features are substantially higher in signal-to-noise than the
quantities we are trying to predict; in particular we are assuming that the photometry
and the spectroscopy is better measured than the astrometry.
This is true for most features for most stars, but it does not hold universally.

\item[representativeness] We assume that the training set constructed from the overlap
of \gaia\ and \apogee\ data sets constitutes a representative sample of stars,
sufficiently representative to train the model for all other stars.
Although this assumption is not terrible, it has a weakness:
The stars at greatest distances and greatest local (angular) stellar crowding have
the least precise \gaia\ parallax measurements, and therefore will effectively get
less weight in the fits performed to train the model.

\item[sparsity] We expect that only a small subset of the full complement of
\apogee\ spectral pixels will be relevant to the prediction of stellar luminosity.
That is, we expect that many of the pixels will or ought to get no weight in the
final model that we use to predict luminosities and distances.

\item[linearity] Perhaps the most restrictive assumption of this work is that
the stellar logarithmic luminosity (or, equivalently, the absolute magnitude) in
the \zmass\ $K_s$~band can be predicted as a \emph{completely linear function} of
the chosen features. We are only assuming this linearity in a small range of stellar
surface gravity $\logg$, but this assumption is strong, and limits strongly the
flexibility or capacity of the model.
We make it to ensure that our method is easy to optimize, and the results are easy
to interpret.

\item[likelihood] We assume that the \gaia\ parallaxes are generated by a particular
stochastic process, in which the difference between the \gaia-Catalog parallax (plus
a small \gaia-recommended offset HOGG CITE) and the true parallax is effectively drawn from a
Gaussian with a width set by the \gaia-Catalog uncertainty on the parallax.
This is the standard assumption in all properly probabilistic \gaia-Mission inferences
to date, but it subsumes a number of related assumptions, like that the \gaia\ noise
model is correct, that the stars are only covariant at negligible levels, and that
there are no significant outliers or large-scale systematics.
\end{description}

In addition to making the above assumptions, we also avoid various practices
with the data that are tempting but lead to strong biases.
For example, we never cut on \gaia\ parallax or parallax signal-to-noise.
The common practices of cutting to parallaxes that are good to 20~percent,
or Catalog parallaxes that are positive, or parallaxes that are smaller or
larger than something, are all practices that will bias results on stellar
collections.
That is, if you cut on parallax or parallax signal-to-noise and you subsequently
take an average of parallaxes for some population, or perform a regression (as we
do here), the results of that average or regression will be strongly biased.
We never cut on parallax or parallax signal-to-noise.
By using a justifiable likelihood function (the ``likelihood'' assumption above),
we can use all of the \gaia\ parallaxes without the low signal-to-noise and
negative parallaxes causing trouble for our regressions, and without the biases
that enter when cuts are made.

Along similar lines, we never assume that the distance is the inverse of the
measured parallax.
In what follows, a star's distance is a latent property of the star, which generates
the \gaia\ Catalog parallax through a noisy process (again, this is
the ``likelihood'' assumption above).
We never take the inverse or the logarithm of the measured parallax at any time.
This is related to the no-cuts point above:
If you take the inverse or logarithm of the parallax, you can't operate safely
on the negative and low signal-to-noise parallaxes, which in turn will require
making cuts on the sample, which will in turn bias the results.

Finally, we never use Lutz--Kelker corrections (HOGG CITE) or distance
posteriors (HOGG CITE). These both involve (implicit or explicit) priors on 
distance.
When multiple stars are combined as we combine them here (below),
use of distance posteriors instead of parallax likelihoods is not just
unjustified, but it also leads to an
effective raising of the distance prior to an enormous power.
That is, the effective prior on a $N$-star inference performed naively
with distance posteriors (made with a weak prior) can end up
bringing into the inference an \emph{exceedingly
strong prior}.
Therefore we don't use any prior-contaminated parallax or distance
inputs to the inference.

\section{Method}

There are not many choices
for building a model of the stars that is both justifiable probabilistically
and consistent with the assumptions stated in the previous Section.
Here we lay out the model and methodology.
We apply the method to real data in the following Sections.

Since the photometry is (by the ``good features'' assumption above)
better measured than the parallaxes, we
presume that the parallax uncertainties dominate
the uncertainties in the luminosity measurements.
For each star $n$ in our training set, we define an inverse-square-root
luminosity $Q_n$ estimate that will be subject to Gaussian noise under these assumptions.
\begin{eqnarray}
Q_n &\equiv& \varpi_n\,10^{0.2\,(K_n - 10)}
\\
\sigma_n &\equiv& \sigma_{\varpi n}\,10^{0.2\,(K_n - 10)}
\quad,
\end{eqnarray}
where
$\varpi_n$ is the \gaia\ Catalog entry for the parallax (in mas) for star $n$,
$K_n$ is the \zmass\ Catalog $K_s$-band magnitude for star $n$,
the $Q_n$ have Gaussian errors of root-variance $\sigma_n$,
and $\sigma_{\varpi n}$ is the \gaia\ Catalog entry for the uncertainty on parallax
for star $n$.
In what follows, the $Q_n$ are the training data.
Naively, the $Q_n$ are related to the absolute magnitudes $M_{Kn}$ for the stars by
\begin{eqnarray}
M_{Kn} &\approx& 5\,\log_{10} Q_n
\quad.
\end{eqnarray}
These would be the absolute magnitudes of the stars under the
``the distance is the inverse of the
measured parallax'' assumption, although we never, in this work, ever make that
assumption, by God.
We construct the $Q_n$ because they are related directly to the absolute magnitudes,
but they (under assumptions) are subject to known Gaussian noise.

The log-likelihood function can be expressed heuristically as
\begin{eqnarray}
Q_n &=& \exp(\theta\cdot x_n) + \mbox{noise}
\\
\chi^2(\theta) &\equiv& \sum_{n=1}^N \frac{[Q_n - \exp(\theta\cdot x_n)]^2}{\sigma_n^2}
\quad ,
\end{eqnarray}
where
the model is that the $\ln Q_n$ can be expressed as a linear combination of the components
of some $D$-dimensional feature vector $x_n$,
$\theta$ is a $D$-vector of linear coefficients,
and $\chi^2(\theta)$ is the negative-log-likelihood for the parameters $\theta$
under the assumption of known Gaussian noise and
that there are $N$ independently measured stars $n$.
This feature vector $x_n$ contains photometry in a few bands, and all 8000-ish pixels
in the pseudo-continuum-normalized \apogee\ spectrum, so $D$ is on the order of 8000.

In addition, we assume that many entries in the $D$-vector will be zero
(the ``sparsity'' assumption).
In order to encourage this, we optimize not $\chi^2$ but a regularized objective function
\begin{eqnarray}
\hat{\theta} &\leftarrow& \argmin_{\theta}\left[\frac{1}{2}\,\chi^2(\theta) + ||\lambda\cdot\theta||_1^1\right]
\quad ,
\end{eqnarray}
where
$\lambda$ is a $D$-vector of regularization parameters (such that we can regularize
different parts of the $x_n$ vectors differently),
and $||x||_1^1$ is the L1-norm or sum of absolute values of the components of $x$.
This kind of regularization adds a convex term to the optimization and leads to
sparse optima.
We will set the value $\lambda$ of the $D$-vector of regularization strengths by a
very coarse cross-validation.

Once the model is optimized, the prediction for a new stellar absolute magnitude
$M_{Km}$ for a star $m$ in the validation or test set is
\begin{eqnarray}
Q_m &=& \exp(\theta\cdot x_m)
\\
M_{Km} &=& 5\,\log_{10} Q_m
\quad ,
\end{eqnarray}
where $x_m$ is the feature $D$-vector for star $m$.
Thus the model can be trained on a training set of stars and applied to make
predictions for a validation set or a test set.

One final detail: We perform all of the fitting in a two-fold train-and-test framework,
in which the data are split into two disjoint subsets, A and B.
The model trained on the A data is used to predict the $Q_m$ values (and hence
the luminosities, distances, and parallaxes) of the B data,
and the model trained on the B data is used to 
predict the $Q_m$ values of the A data.
This ensures that, in the estimate of any individal star's luminosity, distance,
or parallax, none of the \gaia\ data pertaining to that star was used.
This makes the parallax estimates from the spectrophotometric feature vectors
$x_m$ statistically independent of the \gaia\ parallax measurements.
They are not independent globally---\gaia\ data was used to train the model---but
each star's spectrophotometric parallax estimate is independent
of the astrometric estimate from \gaia\ on a star-by-star basis.

\section{Data}

We take the \apogee\ (HOGG CITE) spectral data
from \sdssiv\ (HOGG CITE) \acronym{DR14} (HOGG CITE).
Because we want to make a purely linear model, we restrict our consideration
to a small region in stellar parameter space.
We cut the \apogee\ data down to the range $0<\logg<2.2$, which isolates
stars that are more luminous than the red clump.
The \apogee\ pipeline $\logg$ values (which we use) have uncertainties but
this cut leads to a clean sample of luminous red giants, and it is a cut
that is only on spectral properties of the stars (and not photometry nor astrometry).

Because the \apogee\ target selection is based on \zmass, every \apogee\ star
comes with \zmass photometry.
That is, for each star $n$,
we have near-infrared photometry $J_n$, $H_n$, and $K_n$.
In detail we use from the catalog the entries ACE WHAT SPECIFIC CATALOG ENTRIES.

ACE: We match the \apogee+\zmass\ stars to he \wise\ ACE WHICH CATALOG by ACE WHAT CRITERION?
This gives, fore each matching star $n$,
mid-infrared photometry $W_{1n}$ and $W_{2n}$ at 3.6 and 4.5\,$\mu$m
In detail we use the ACE WHAT SPECIFIC CATALOG ENTRIES?

We match this full-match catalog to the \gaia\ \acronym{DR2} (HOGG CITE) by what criteria?

We take from the \gaia\ \acronym{DR2} Catalog the photometric data $G_n$
(\code{phot_g_mean_mag}) and $G_{Bn}-G_{Rn}$ (\code{bp_rp}), which will
become part of the feature vector $x_n$.
We do not use the entire \gaia\ \acronym{DR2} Catalog but instead the subset
that meets ACE WHAT ADDITIONAL PHOTOMETRIC CUTS?

We need complete feature-vector information for all stars.  For this
reason, we define the Parent Sample to be the set of all stars that
meet the \apogee\ and \gaia\ cuts and also
have the complete set of photometry: $G_n$, $G_{Bn}-G_{Rn}$, $J_n$,
$H_n$, $K_n$, $W_{1n}$, and $W_{2n}$ and also an apogee spectrum.
This Parent Sample contains ACE HOW MANY STARS? and is shown in HOGG WHAT FIGURES?

Every Parent Sample object gets, in addition, a randomly assigned binary
label (A or B).
This is used for two-fold validation and jackknife.
In short, we will use the model trained on the A data to assign luminosities
and distances to the B data and \foreign{vice versa}.
The Parent Sample A has HOGG entries and Parent Sample B has HOGG entries.

For training, we need A and B Training Sets.
We define the Training Set stars to be Parent Sample stars that also
have a measured \gaia\ parallax.
The Training-Set stars, in addition to meeting all Parent-Sample cuts,
also must meet ACE WHAT ADDITIONAL CUTS to ensure that the parallax
measurements are good.
But---as we have emphasized above---we do not cut ever on parallax or
parallax error or their ratio. For this reason, most of the training set
do not have significantly measured parallaxes and HOGG percent even have
negative parallaxes!
But we will perform our training such that it will be unbiased in these
circumstances.
Indeed it would be necessarily biased if ever we did cut on parallax or
parallax signal-to-noise.

In detail,
for each star $n$ in the Training Sets, we take from \gaia\ DR2 we the parallax $\varpi_n$
and its uncertainty $\sigma_{\varpi n}$, which will be used in
constructing the inverse-root-luminosity $Q_n$ and associated
uncertainty $\sigma_n$.
Importantly, to each \gaia\ parallax measurement we add a positive
offset of $0.029$\,mas to adjust for the under-estimation of
parallaxes reported by the \gaia\ Collaboration (HOGG CITE LINDEGREN).
The training sets are shown in HOGG WHAT FIGURES?

For every star $n$ in the full Parent Sample we construct the feature
vector $x_n$ as HOGG PUT EQUATION HERE.
The feature vectors live in a $D$-dimensional space where $D=$HOGG.
For every star $n$ in either of the Training Sets we also construct a
regression datum $Q_n$ and uncertainty $\sigma_n$ according to HOGG WHAT EQUATION ABOVE?

\section{Results and validation}

HOGG: We do 2-fold train-and-test to make results independent of \gaia.

We compute our precision how? Figures.

We set the regularization parameters how? And why?

Brief comment on the flexibility of the photometric model and interpretation of the photometric coefficients. Maybe plot some photometric properties vs metallicity or dust?

\section{Discussion}

What did we do and what's cool?

Why did we make the assumptions we made and what did they gain us and cost us?

(In addition: what did we assume about the \apogee\ data: Good $\logg$ values.

What is this useful for and how do you use them safely?

(Mention independence of every result from the \gaia\ astrometry.)

Come back to SDSS-V and show the rotation curve of the Galaxy.

\acknowledgements
It is a pleasure to thank
  Andy L. Jones (HOGG),
  Daniel Michalik (\acronym{ESTEC}), and
  Melissa Ness (Columbia)
for help with this project.
This project was developed in part at the
2018 \acronym{NYC} Gaia Sprint, hosted by the Center for Computational Astrophysics of
the Flatiron Institute in New York City.

This work has made use of data from the European Space Agency (ESA) mission
\gaia\ (\url{https://www.cosmos.esa.int/gaia}), processed by the \gaia\ Data
Processing and Analysis Consortium (\acronym{DPAC},
\url{https://www.cosmos.esa.int/web/gaia/dpac/consortium}). Funding for the
\acronym{DPAC}
has been provided by national institutions, in particular the institutions
participating in the \gaia\ Multilateral Agreement.

\wise\ Acknowledgement?

\project{\acronym{SDSS-IV}} ack.

\begin{thebibliography}{}
\bibitem[Astropy Collaboration et al.(2013)]{2013A&A...558A..33A} Astropy Collaboration, Robitaille, T.~P., Tollerud, E.~J., et al.\ 2013, \aap, 558, A33 
\bibitem[Bertin \& Arnouts(1996)]{1996A&AS..117..393B} Bertin, E., \& Arnouts, S.\ 1996, \aaps, 117, 393 
\bibitem[Corrales(2015)]{2015ApJ...805...23C} Corrales, L.\ 2015, \apj, 805, 23
\bibitem[Ferland et al.(2013)]{2013RMxAA..49..137F} Ferland, G.~J., Porter, R.~L., van Hoof, P.~A.~M., et al.\ 2013, \rmxaa, 49, 137
\bibitem[Hanisch \& Biemesderfer(1989)]{1989BAAS...21..780H} Hanisch, R.~J., \& Biemesderfer, C.~D.\ 1989, \baas, 21, 780 
\bibitem[Lamport(1994)]{lamport94} Lamport, L. 1994, LaTeX: A Document Preparation System, 2nd Edition (Boston, Addison-Wesley Professional)
\bibitem[Schwarz et al.(2011)]{2011ApJS..197...31S} Schwarz, G.~J., Ness, J.-U., Osborne, J.~P., et al.\ 2011, \apjs, 197, 31  
\bibitem[Vogt et al.(2014)]{2014ApJ...793..127V} Vogt, F.~P.~A., Dopita, M.~A., Kewley, L.~J., et al.\ 2014, \apj, 793, 127  
\end{thebibliography}

%% This command is needed to show the entire author+affilation list when
%% the collaboration and author truncation commands are used.  It has to
%% go at the end of the manuscript.
%\allauthors

%% Include this line if you are using the \added, \replaced, \deleted
%% commands to see a summary list of all changes at the end of the article.
%\listofchanges

\end{document}

% End of file `sample62.tex'.
